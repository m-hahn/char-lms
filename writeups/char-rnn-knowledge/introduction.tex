\section{Introduction}
\label{sec:introduction}


Recurrent neural networks \cite[RNNs,][]{Elman:1990}, in particular
in their Long-Short-Term-Memory variant
\cite[LSTMs,][]{Hochreiter:Schmidhuber:1997}, are the current
workhorse of natural language processing. RNNs, often
pre-trained on the simple \emph{language modeling} objective of
predicting the next symbol in natural text, are a crucial
component of state-of-the-art architectures for machine
translation, natural language inference and text categorization
\cite{Goldberg:2017}.

RNNs are very general devices for sequence processing, assuming little
prior bias. Moreover, the simple prediction task they are trained on
in language modeling seems well-aligned with the core role prediction
plays in cognition \cite[e.g.,][]{Bar:2007,Clark:2016}. RNNs have thus
long attracted researchers
interested in language acquisition and processing. Their recent successes in
realistic large-scale tasks has strongly rekindled this interest
\cite[see, e.g.,][and references there]{Frank:etal:2013,Lau:etal:2017,Kirov:Cotterell:2018,McCoy:etal:2018,Pater:2018}.

The standard pre-processing pipeline of modern RNNs assumes that the
input has been tokenized into word units that are pre-stored in the
RNN vocabulary. This is a reasonable practical approach, but it makes
simulations less interesting from a linguistic point of view. First,
discovering words is one of the major challenges a learner faces, and
by pre-encoding them in the RNN we are facilitating its task in an
unnatural way (not even the staunchest nativists would take specific
word dictionaries to be part of our genetic code). Second, assuming a
unique tokenization into a finite number of discrete word units is in
any case problematic. The very notion of what counts as a word in
languages with a rich morphology is far from clear
\cite[e.g.,][]{Bickel:Zuniga:2017}, and, universally, mental lexicons
are probably organized into a not-necessarily-consistent hierarchy of
units at different levels: morphemes, words, compounds, constructions,
etc.~\cite[e.g.,][]{Goldberg:2005}. Indeed, it has been suggested that
the notion of word cannot even be meaningful defined on a
cross-linguistic level \cite{Haspelmath:2011}.

Motivated by these considerations, we study here RNNs that are trained
without any notion of word units in their input or in their
architecture. In particular, we train RNNs as \emph{character-level
  neural language models}
\cite[CNLMs,][]{Mikolov:etal:2011,Sutskever:etal:2011,DBLP:journals/corr/Graves13}. Moreover,
we remove whitespace from their input, so that, like children learning
a language, they don't have access to explicit cues to
wordhood.\footnote{We do not erase punctuation marks, reasoning that
  they have a similar function to prosodic cues in spoken language.}
This setup is almost as \emph{tabula rasa} as it goes. By using
unsegmented orthographic input (and assuming that, in the alphabetic
writing systems we work with, there is a reasonable correspondence
between letters and phonetic segments), we are only postulating that
the learner figured out how to segment the continuous speech stream
into phonological units, an ability children already possess few
months after birth
\cite[e.g.,][]{Maye:etal:2002,Kuhl:2004}.

After training the networks on the unsupervised character-level
language modeling task, we then task on a bank of morphological,
syntactic and semantic tests, in German, Italian and
English.\footnote{We focus in this first study on these languages due
  to resource availability and ease for us to construct the relevant
  benchmarks in them.}  These tasks intuitively require a model to
have developed a latent ability to parse characters into word-like
units associated to morphological, grammatical and broadly semantic
features. We find that our RNNs pass most of the tests, thus
suggesting that the regularities they encounter in their input at
training time suffice to let them extract and use lexical knowledge
despite the lack of word-centric priors. In a final experiment, we
thus proceed to test more directly whether the hidden activities of
the RNNs is tracking information about word boundaries, finding that
indeed they do.  In the conclusion, we discuss the broader
implications and limits of our simulations.\footnote{Upon publication,
  we will make our input data, test sets and pre-trained model
  available.}

% probe them with phonological, lexical,
% morphological, syntactic and semantic tests in English, German and
% Italian. Our results show that near-\emph{tabula-rasa} CNLMs acquire
% an impressive spectrum of linguistic knowledge at various levels.
% This in turn suggests that, given abundant input (large Wikipedia
% dumps), a learning device whose only prior architectural bias consists
% in the LSTM memory cell implicitly acquires a variety of linguistic
% rules that one would intuitively expect to require much more prior
% knowledge.
