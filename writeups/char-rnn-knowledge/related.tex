\section{Related work}
\label{sec:related}

\paragraph{On the primacy of (orthographic) words} Our work is
inspired by linguistic studies that have proposed, from different
perspectives and theoretical backgrounds, that words, at least as
delimited by whitespace in some written languages, are neither
necessary nor sufficient units for linguistic analysis. This ranges
from the very direct claim of \newcite{Haspelmath:2011} that there is
no good criterion to define what a word is cross-linguistically
\cite[see also][who address specifically the notion of prosodic
word]{Schiering:etal:2010}, to the difficulty of characterizing a
consistent notion of word in polysynthetic languages
\cite{Bickel:Zuniga:2017}. Children are only rarely exposed to words
in isolation during learning \cite{Tomasello:2003}.\footnote{While
  single-word utterances are not uncommon in child-directed language,
  they are still rather the exception than the rule, and many
  important words, such as determiners, will never occur in isolation
  \cite{Christiansen:etal:2005}.} Several linguists have moreover
suggested that the units that speakers eventually store in their
lexicon are of variable size, both smaller and larger than
conventional words \cite[e.g.,][]{Jackendoff:2002,Goldberg:2005}. Our
study shows that a powerful sequence learning device, such as an RNN
with LSTM cells, can learn, from naturally occurring language data, to
capture several linguistic phenomena that appear to be word-mediated
without overt word-boundary information and lacking an internal data
structure for a word vocabulary. The model, moreover, develops during
learning units that track word-like boundaries, despite its lack of an
explicit lexicon. Our results thus contribute to the theoretical
debate on word primacy, suggesting that learning or representational
word priors might not be as crucial as typically assumed.

\paragraph{Character-based neural language models} have received attention in the last
decade because of their greater generality, and because, intuitively, they should be able to
use cues, such as morphological information, that word-based models
miss by design. Early studies such as \newcite{Mikolov:etal:2011},
\newcite{Sutskever:etal:2011} and \newcite{DBLP:journals/corr/Graves13} established
that CNLMs (trained with whitespace where relevant) are in general not
as good at language modeling as their word-based counterparts, but lag
only slightly behind (note that character-level sentence prediction
involves a much larger search space than predicting at the word
level). \newcite{Sutskever:etal:2011} and \newcite{DBLP:journals/corr/Graves13}
presented informal qualitative analyses showing that CNLMs are
learning basic linguistic properties of their input. The latter, who
trained LSTM-based models, also showed that they can keep track, to
some extent, of hierarchical structure. In particular, they are able
to correctly balance parentheses when generating text.

% Our aim here is to understand to what extent CNLMs trained on
% unsegmented input learn various linguistic constructs. % This differs
% from
Most recent work in the area, has focused on \emph{character-aware}
architectures combining character- and word-level information to
develop state-of-the-art language models that are also effective in
morphologically rich languages \citep[see, e.g.,][and references
there]{Bojanowski:etal:2016,Kim:etal:2016,Gerz:etal:2018}. For
example, the influential model of Kim and colleagues performs
prediction at the word level, but uses a character-based convolutional
network to generate word representations. Other work focuses on
splitting words into morphemes with character-level RNNs, with an
explicit segmentation objective \cite[e.g.,][]{Kann:etal:2016}. Closer
to our approach, early work by \newcite{Christiansen:etal:1998}
trained a RNN on phoneme-level language modeling of transcribed
child-directed speech with tokens marking utterance boundaries, and
found that the network had learn to segment the input by predicting
the utterance boundary symbol also at work edges.
% , as
% opposed to our interest in probing what the network implicitly learned
% about morphemes and other units through generic language
% modeling.% Moreover, these networks are not exposed to constituents
% larger than words.


\paragraph{Probing linguistic knowledge of neural language models}
Extensive work probes the linguistic properties of word-based neural
language models, as well as more complex architectures such as
sequence-to-sequence systems: see, e.g.,
\newcite{Li:etal:2016,Linzen:etal:2016,Shi:etal:2016,Adi:etal:2017,Belinkov:etal:2017,Kadar:etal:2017,Hupkes:etal:2017,Conneau:etal:2018,Ettinger:etal:2018,Linzen:etal:2018}.

Early work by Jeffrey Elman is close in spirit to ours. In particular,
\newcite{Elman:1990} reported a proof-of-concept experiment on
implicit learning of word segmentation. More recently,
\newcite{Sennrich:2017} explored the grammatical properties of
character- and subword-unit-level models that are used as components
of a machine translation system. He concluded that current
character-based decoders generalize better to unseen words, but
capture less grammatical knowledge than subword units. Still, his
character-based systems lagged only marginally behind the subword
architectures on grammatical tasks such as handling agreement and
negation. \newcite{DBLP:journals/corr/RadfordJS17} also studied CNLMs
with focus on understanding their properties in the domain of
sentiment analysis. \newcite{Godin:etal:2018} investigated the rules
implicitly used by supervised character-aware neural morphological
segmentation methods, finding in particular that the networks discover
linguistically sensible patterns. More closely related to our goals,
\newcite{Alishahi:etal:2017} probed the linguistic knowledge induced
by a neural network that receives unsegmented acoustic input. Focusing
on phonology, they find that phonemic information is processed at a
fine-grained level by the lower layers of the model, whereas higher
layers are sensitive to more abstract information. Their model is not
directly comparable to ours, since it uses a considerably more complex
architecture and it is trained on multimodal
data. \newcite{Kementchedjhieva:Lopez:2018} recently presented a
related study probing the linguistic knowledge of plain
character-level neural language models (trained with whitespace in the
input). Their results are aligned with ours, as they show that these
models have knowledge of lexical and morphological structure, and they
capture morphosyntactic categories as well as constraints on possible
morpheme combinations. One of their most intriguing results is that
the model tracks word/morpheme boundaries through a single unit. We
replicated this finding for our network, as we further discuss in
Section \ref{sec:segmentation}.
%We were not able to find a similar sparse
%encoding of morpheme- or word-boundaries in our network (the results
%we will report in Section \ref{sec:segmentation} suggest that our
%model is also tracking boundaries in its hidden state, but through a
%distributed code). The most obvious difference between our experiment
%and that of Kementchedjhieva and Lopez is that they keep whitespace in
%the input training.  However, they observe that their
%morpheme-tracking unit is not simply predicting white space,
%suggesting that the encoding difference between our model and theirs
%cannot be (entirely) explained away by the difference in training
%input. As we do not attempt here to characterize the inner dynamics of
%our network, we leave a systematic comparison to future work.


% They do not explore syntactic or semantic
% knowledge, and they limit their study to English. Moreover, they
% trained their models on input with whitespace, thus providing the
% model with a major (and cognitively artificial) cue to word
% boundaries.
