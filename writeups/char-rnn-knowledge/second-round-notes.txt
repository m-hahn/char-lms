MANDATORY CHANGE: The reviewers agree that the word boundary
finding is the most interesting part of the paper. But you
don't show what word recognition rates are on actual
text.  Please do the following additional experiment:
"duplicate" Table 6 and also report word recognition rates
on real text. I guess F1 would be a standard and clear
measure. (90% for English on the artificial data set doesn't
tell me how well your model segments real text.)

> done


line 995: perhaps add that the error analysis is for the
"single unit" classifier (can be inferred only from the
result Table 6, where German is the language w/ the
single-unit classifier reaching highest accuracy).

> done

link 962: "aditional dataspoints"

> done


ln 070-073: I'd like a citation or two substantiating the claim that this is
standard.

> 

ln 192: What is the connection between the prior work in this paragraph and
your work?

> 

ln 222: "Radford et al. (2017) focused on CNLMs deployed in the domain of
sentiment analysis." The fact that they were working on sentiment analysis
doesn't seem relevant to the current discussion. What did they learn about
CLNMs? Why is this paper relevant?

> 

ln 242: Here, too, I miss a brief statement of why your questions are the
logiacl next step to take.

> 

ln 689 "For the n-gram baseline, we only counted occurrences of the
prepositional phrase, omitting sentences.": This use of "omitting" makes it
sound like were tested on both PPs and sentences. But surely that's not
right?

> done

ln 737 "We required moreover the -a and -o forms of a noun to be reasonably
balanced in frequency (neither form is twice more frequent than the other),"
Why? How does this decision contribute/relate to the overall research goal?

>

ln 947 "Again, in left-to-right processing, the unit has a tendency to
immediately posit boundaries when frequent function words are encountered."
This suggests a way in which NN processing is quite unlike human processing,
which can recover from incorrect decisions in light of further information.

>

ln 1162 "Intriguingly, our CNLMs captured a range of lexical phenomena
without anything resembling a word dictionary": Your results seem to
suggest they build one internally, though! I think you’re confusing
levels of abstraction here. No one argues that there are specific
neurons for each word in the human brain.

>

ln 013 reached -> has reached
> done

ln 135 is -> are
> done

ln 178 "that CLNMs hierarchical structure": Missing 'model'?
> done

ln 212 work -> word
> done

ln 559 The sentence final . in the middle of a parenthetical that is itself
embedded in a sentence is awkward.
> TODO change to a semicolon?

ln 724 "Italian has a relatively extended paradigm": This led me to expect
lots of suffixes, not lots of stems. I'd suggest revising this.
> done

ln 822 capable to track -> capable of tracking
> done

ln 1042, 1048: ' -> ` x2
> done

ln 1121 input, -> input
> ?

ln 1126 latter ability -> latter's ability
> done




The authors acknowledge and explain the low results of the RNN, and add a
footnote that this might be due to resource availability. However, I am
still puzzled, as the authors. While I agree that leaving a further
investigation on the low RNN performance to future work, this makes me think
that the paper might be better off dropping the RNN model completely. What's
the real benefit of keeping it, other than showing it does not work so well
as the LSTM?

>

In fact, it is still pretty bad, particularly in Table 5 in-domain vs
cross-domain giving the same performance? Also, by keeping the RNN, one
could argue why not instead investigate GRUs as well, which is a bit besides
the point of the paper. On the word class prediction task, the RNN has a
huge standard deviation, it is very unstable. I'd say drop the RNN, and
instead, add a random baseline throughout the paper, and explain the
autoencoder baseline (regarding random: even though for most tasks the
chance-level is just 50%) -- see comment on missing autoencoder setup below.

> 

As looking 'beyond' the threshold meant getting confirmation and additional
very interesting results, I would appreciate if the paper could openly say
so (e.g., in a footnote? appendix?)  I think this *is* an educative aspect,
which is now only visible to reviewers. Without lowering the threshold this
interesting analysis (including the quantitative analysis in Figure 2) would
not be part of the paper. Is there a way to 'keep' this?

> TODO


- "soft" in abstract: why "soft" word boundaries? Evaluation (Table 6)
assumes a hard word boundary prediction tasks, hence consider to drop
"soft", which is not explained.

>

- experimental setup: the paper now introduces an autoencoder as baseline.
As mentioned above, I would propose to explicitly add an even simpler
baselines (random or majority). Moreover, there is a large space of
possibilities for autoencoders (sparse, overcomplete...) and the current
version entirely misses to describe the autoencoder exp. setup. it could be
added as a paragraph at the end of Section 4.

> Added footnote for architectural details of autoencoder.

> TODO let's make sure the reader understands what random baseline is. Is there a citation for LSTM autoencoders?

- "the very notion of what counts as a word" - this is in fact an important
general question, there must be a less recent reference, before 2017?

>

- Table 2 + 3 presentation of results: I typically find it easier if the
baseline is at the start of the table, then models, then other (like the
"subs" model, which is not strictly comparable, could be be moved to the
bottom of the table separated by a hline). This "teasing apart" would make
the tables more readable, I believe.

>

- The plural noun number classification experiment (page 5) has a
well-motivated setup, the test sets (-r, and umlaut change) seem to have
been set up in a way to consist of the more difficult tasks (while training
on the more regular forms -n/-s/-e), which the last part seems to hint at
"generalization is not completely abstract". If this is in line of what the
authors had in mind when designing the experiments, this motivation could be
made overt in favor of strengthening the motivation of the setup (because
otherwise once could be inclined to ask for more permutations of these
experiments..). ;)

>

- line 167: than predicting -> than prediction
>
- line 178: missing verb in CNLMs sentence
>
- line 467: "successful outperforming in most cases" - split in multiple
sentences. "... It outperforms.."
>
- line 473: "near-random" -> add random baseline to table
> done
- line 514: "controlled for character length" - I might have missed how this
has been done
>
- line 570: comma in number (in a few other places as well)
> done (maybe still missing a few)

- line 936: nice example with Hauptaufgabe!
- line 1042: ' -> `
> done

- discussion section: consider adding a few new lines (first paragraph) to
make it more readable, e.g., in line 1124 and 1131

>

- consider adding a reference; an earlier study that proposes the use of
segmentation-free NLP (Schütze, 2017):
http://aclweb.org/anthology/E/E17/E17-1074.pdf

>

- "and a dummy variable coding word-final position" - this part remains
unclear to me. How exactly twas this per-neuron correlation done, how was
this dummy-variable derived?

> I made the definition of the dummy variable more explicit. Feel free to change.

However, I did not feel that the overall setting of removing word boundaries
is sufficiently argumented or justified. The motivation seems to be that
this is how humans experience language. However, written text is already
quite different from human speech and I wouldn't say it becomes
substantially more similar by removing the word boundaries. If punctuation
is kept (even dashes in words), under the argument of encoding prosody
information, then surely word boundaries also indicate certain prosodic
features. Instead, the whole setting in the paper, in which neural
character-based language models are investigated, does not match any of the
normal settings in which a neural character model would be applied. If the
goal was to investigate LSTM performance on human speech, then the
experiments could have been performed on actual audio input, using aligned
transcriptions as the targets. If the goal was to investigate LSTM
performance on text, then I would suggest including the results from a
word-delimited character LM as well.

>

I would have expected the probing tasks to either:
a) compare different language model architectures and conclude which ones
are better at which tasks, or
b) analyse the performance of one language model architecture across
different phenomena and conclude what is it good at, where are its
weaknesses and what does it mean for future work.
Unfortunately, I did not see either of these in the current paper. The main
conclusions seem to be that LSTMs still perform reasonably well after
introducing the artificial constraint of removing word boundaries, and I'm
not sure what this shows or how this will be useful.

>

It is fairly expected that language models would learn to encode information
such as word class, number or gender. They are specifically trained to
predict the surrounding words/characters, which requires this information
for agreement. However, it is less clear to which extent the supervised
experiments actually show this property - given that a separate supervised
component is trained on top of the LM representations, it could
theoretically be picking up on useful feature correlations instead of the
desired property directly.

>

Several of the chosen baselines seem to be particularly poor in this paper.
Plain RNNs are not used in practice any more, as their lower performance is
very well established. And the ngram model is not really a proper ngram
language model - L577 says it's basically just picking the most frequent
case in the data, based on one word of context. Giving the model only one
word of context when several of the tasks are specifically constructed to
require 2 or more words of context seems very unfair. I do not see why a
proper ngram language model could not have been used (e.g. a 5-gram model
with Kneser-Ney smoothing). A truly useful investigation would be to include
some of the more recent state-of-the-art language model architectures into
the comparison.

>

The finding of the word boundary neurons is probably the strongest part of
the paper.
However, as pointed out in the paper as well, this finding has already been
shown in previous work (Kementchedjhieva and Lopez, 2018).
It is unclear what the differences are and what is the novel contribution in
this section.

>

The argument that word boundaries are not necessary in language is somewhat
weakened by the finding that even without boundary information LSTMs still
adapt by explicitly learning to detect word boundaries internally.

>

Many of the experiments seem to be very heuristically constructed with not
much motivation provided. This includes the limited choice of languages for
investigation, the words that are selected for training or testing in
various experiments, various filters based on frequency and suffixes, etc.
Making the experiments more general and providing better explanations for
the remaining choices would make the paper stronger.

>

Spelling errors:
L178: I think a word is missing
> done
L180: Unnecessary comma
> ?
L963: dataspoints
> done


