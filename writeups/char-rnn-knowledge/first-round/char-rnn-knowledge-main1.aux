\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Elman:1990}
\citation{Hochreiter:Schmidhuber:1997}
\citation{Goldberg:2017}
\citation{Bar:2007,Clark:2016}
\citation{Frank:etal:2013,Lau:etal:2017,Kirov:Cotterell:2018,McCoy:etal:2018,Pater:2018}
\citation{Bickel:Zuniga:2017}
\citation{Goldberg:2005}
\citation{Mikolov:etal:2011,Sutskever:etal:2011,DBLP:journals/corr/Graves13}
\citation{Maye:etal:2002,Kuhl:2004}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{Mikolov:etal:2011}
\citation{Sutskever:etal:2011}
\citation{DBLP:journals/corr/Graves13}
\citation{Sutskever:etal:2011}
\citation{DBLP:journals/corr/Graves13}
\citation{Bojanowski:etal:2016,Kim:etal:2016,Gerz:etal:2018}
\citation{Kann:etal:2016}
\citation{Li:etal:2016,Linzen:etal:2016,Shi:etal:2016,Adi:etal:2017,Belinkov:etal:2017,Kadar:etal:2017,DBLP:journals/corr/LiMJ16a,DBLP:conf/ijcai/HupkesZ18,Conneau:etal:2018}
\citation{Elman:1990}
\citation{Sennrich:2017}
\citation{DBLP:journals/corr/RadfordJS17}
\citation{Godin:etal:2018}
\citation{Alishahi:etal:2017}
\citation{Kementchedjhieva:Lopez:2018}
\newlabel{sec:related}{{2}{2}{Related work}{section.2}{}}
\citation{merity2018analysis}
\citation{cotterell2018all}
\citation{DBLP:journals/corr/Graves13}
\citation{Gulordava:etal:2018}
\newlabel{sec:setup}{{3}{3}{Experimental setup}{section.3}{}}
\newlabel{tab:lm-results}{{1}{3}{Performance of language models. For CNLMs, we report bits-per-character (BPC). For WordNLMs, we report perplexity}{table.1}{}}
\newlabel{sec:experiments}{{4}{3}{Experiments}{section.4}{}}
\citation{harris-distributional-1954,saffran-word-1996}
\citation{cohen-algorithm-2001,feng-accessor-2004}
\citation{sun-chinese-1998}
\citation{saffran-word-1996,feng-accessor-2004}
\newlabel{fig:char-clustering}{{1}{4}{Clustering of German character embeddings (alphabetic characters only)}{figure.1}{}}
\newlabel{sec:phonotactics}{{4.1}{4}{Discovering phonotactic constraints}{section*.4}{}}
\newlabel{tab:phonotactics-results}{{2}{4}{Likelihood ratio between acceptable and unacceptable bigrams, with arithmetic (AM) and geometric (GM) means. Values $>1$ in bold}{table.2}{}}
\newlabel{sec:segmentation}{{4.2}{4}{Word segmentation}{subsection.4.2}{}}
\citation{goldwater-bayesian-2009}
\citation{brent-efficient-1999}
\citation{goldwater-bayesian-2009}
\citation{goldwater-bayesian-2009}
\citation{petrov2007improved}
\newlabel{tab:segmentation-results}{{3}{5}{Percentage precision, recall, and F1 on test set word segmentation}{table.3}{}}
\newlabel{tab:segmentation-results-brent}{{4}{5}{Word segmentation results (percentage precision/recall/F1) on our test partition of the Brent corpus for our CNLM-based model and the Bayesian approach of \newcite {goldwater-bayesian-2009}. Following them, we evaluate at the level of tokens, the lexicon of induced word types, and boundaries}{table.4}{}}
\citation{de2006generating,mcdonald2013universal}
\citation{Mikolov:etal:2013a}
\newlabel{fig:syntax-depth}{{2}{6}{PMI between left and right contexts, as estimated by the LSTM CNLM in German, organized by syntactic hierarchical distance between subsequent characters (with bootstrapped 95 \% confidence intervals)}{figure.2}{}}
\newlabel{sec:categories}{{4.3}{6}{Discovering morphological categories}{subsection.4.3}{}}
\newlabel{tab:pos-results}{{5}{6}{Word class accuracy, with standard errors. `\emph {subs.}' marks in-vocabulary subset evaluation}{table.5}{}}
\newlabel{tab:number-results}{{6}{7}{German number classification accuracy, with standard errors computed from 200 runs}{table.6}{}}
\newlabel{sec:dependencies}{{4.4}{7}{Capturing syntactic dependencies}{subsection.4.4}{}}
\newlabel{fig:german-syntax}{{3}{8}{Accuracy on the German syntax tasks, as a function of the number of intervening elements}{figure.3}{}}
\citation{Zweig:Burges:2011}
\newlabel{tab:ital-agr-results}{{7}{9}{Italian agreement results}{table.7}{}}
\newlabel{sec:semantics}{{4.5}{9}{Semantics}{subsection.4.5}{}}
\citation{woods2016exploiting}
\citation{melamud2016context2vec}
\citation{Mikolov:2012}
\citation{zweig2012computational}
\citation{zhang2016top}
\citation{DBLP:journals/corr/abs-1301-3781}
\citation{woods2016exploiting}
\citation{melamud2016context2vec}
\citation{Mikolov:2012}
\citation{zweig2012computational}
\citation{zhang2016top}
\citation{DBLP:journals/corr/abs-1301-3781}
\citation{woods2016exploiting}
\citation{melamud2016context2vec}
\citation{Gulordava:etal:2018}
\citation{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016}
\bibdata{marco,michael}
\bibstyle{acl_natbib}
\newlabel{tab:msr-completion-results}{{8}{10}{Results on MSR Sentence Completion. For our models (top), we show accuracies for Wikipedia/in-domain training. We compare with language models from prior work (left): Kneser-Ney 5-gram model \cite {Mikolov:2012}, Word RNN \cite {zweig2012computational}, Word LSTM and LdTreeLSTM \cite {zhang2016top}. We further report models incorporating distributional encodings of semantics (right): Skipgram(+RNNs) from \newcite {DBLP:journals/corr/abs-1301-3781}, the PMI-based model of \citet {woods2016exploiting}, and the context-embedding based approach by \citet {melamud2016context2vec}}{table.8}{}}
\newlabel{sec:discussion}{{5}{10}{Discussion}{section.5}{}}
