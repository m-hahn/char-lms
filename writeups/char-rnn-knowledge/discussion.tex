\section{Discussion}
\label{sec:discussion}

We probed the linguistic information induced by a character-level LSTM
language model trained on unsegmented text. The model was found to
possess implicit knowledge about a range of word-mediated phenomena,
such as lexical category distinction, morphological features such as
number, and syntactic agreement phenomena. A more standard model
pre-initialized with a word vocabulary and reading tokenized input was
in general superior, but the performance of our agnostic model did not
generally lag much behind, suggesting that the word prior is helpful
but not fundamental. The performance of a character-level RNN was less
consistent than that of the LSTM, suggesting that the ability of the
latter to track information across longer time stretches is important
to extract the correct linguistic generalizations. N-gram baselines
relying on adjacent-string statistics failed almost everywhere,
showing that the neural models are tapping into somewhat more abstract
patterns than local co-occurrence statistics. We then proceeded to
directly test whether our character-based model has implicitly learned
information about word boundaries, founding that indeed it
has.

Our results are preliminary in many ways. The tests we used are
generally quite simple. We did not attempt, for example, to model
long-distance agreement in presence of distractors, a challenging task
even for word-based models and humans
\citep{Gulordava:etal:2018}. Also, as the failure of the model to
correctly generalize number features across German nominal classes
characterized by very different morpho-phonological ways to mark
plural suggests, further work should probe to what extent our model is
capturing genuine abstract linguistic features, as opposed to relying
on more superficial heuristics. Still, taken together, the results
suggest that a large corpus, combined with the weak priors encoded in
an LSTM, might suffice to learning generalizations about word-mediated
linguistic processes, without internal bias for or external cue to
wordhood.

Nearly all contemporary linguistics recognizes a central role to the
lexicon \cite[see, e.g.,][for very different
perspectives]{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016}. Linguistic
formalisms always assume that such lexicon is essentially a dictionary
of words and possibly other units.  Very intriguingly, our CNLMs
captured a range of phenomena that require lexical information
\emph{without} anything resembling a dictionary. Any information the
CNLM might acquire about units larger than characters must be stored
in its recurrent weights. This suggests a radically different view of
lexical knowledge as implicitly encoded in a distributed memory, that
we will strive to characterize more precisely and test in future work.

Another important direction to extend this work concerns the input. To
what extent does the ability of the CNLM to extract the correct
generalizations depend on the huge amount of data it is fed?  Will the
word prior become more important when learning from much smaller
corpora? In terms of a comparison with human learning, we must also
remark that the Wikipedia text our CNLMs receive as input is very far
from what children hear, and future work should explore
character/phoneme-level learning from corpora of child-directed
speech. Still, arguably, by feeding the RNNs directly long-winded
grown-up prose, we are making the job of identifying basic
constituents harder than it might be from the short utterances
characterizing early child-directed speech \cite{Tomasello:2003}.


One of our original motivations for not assuming word primitives is that a rigid word notion is problematic both cross-linguistically (cf.~polysynthetic and agglutinative languages) and within a single linguistic system (cf.~the common  view  that the lexicon hosts units at different levels of the linguistic hierarchy, from  morphemes to large syntactic constructions). Our brief analysis of the CNLM over- and undersegmentations suggested that it is indeed capable to flexibly store information about units at different levels. However, this topic  remained largely unexplored, and we plan to tackle it in future work.



