\section{Discussion}
\label{sec:discussion}

We probed the linguistic information induced by a character-level LSTM
language model trained on unsegmented text. The model was found to
possess implicit knowledge about a range of intuitively word-mediated
phenomena, such as sensitivity to lexical categories and syntactic and
shallow-semantics dependencies. A model initialized with a word
vocabulary and fed tokenized input, was in general superior, but the
performance of the word-less model did not lag much behind, suggesting
that word priors are helpful but not strictly required. A
character-level RNN was less consistent than the LSTM, suggesting that
the latter's ability to track information across longer time
spans is important to make the correct generalizations. The
character-level models consistently outperformed n-gram controls,
confirming they are tapping into more abstract patterns than local
co-occurrence statistics. As a first step towards understanding
\emph{how} character-level models handle supra-character phenomena, we
searched and found specialized boundary-tracking units in them. These
units are not only and not always sensitive to word boundaries, but
also respond to other salient items, such as morphemes and multi-word
expressions, in accordance with an ``emergent'' and flexible view of
the basic constituents of language \cite{Schiering:etal:2010}. Our
results are preliminary in many ways. Out tests are relatively
simple. We did not attempt, for example, to model long-distance
agreement in presence of distractors, a challenging task even for
word-based models and humans \citep{Gulordava:etal:2018}. The results
on number classification in German suggest that the models might not
be capturing linguistic generalizations of the correct degree of
abstractness, settling for shallower heuristics. Still, as a whole,
our work suggests that a large corpus, combined with the weak priors
encoded in an LSTM, might suffice to learn generalizations about
word-mediated linguistic processes without a hard-coded word lexicon
or explicit wordhood cues.

Nearly all contemporary linguistics recognizes a central role to the
lexicon \cite[see, e.g.,][for very different
perspectives]{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016}. Linguistic
formalisms assume that the lexicon is essentially a dictionary
of words, possibly complemented by other units, not unlike the list of
words and associated embeddings in a standard word-based
NLM. Intriguingly, our CNLMs captured a range of lexical phenomena
\emph{without} anything resembling a word dictionary. Any information
a CNLM might acquire about units larger than characters must be stored
in its recurrent weights. This suggests a radically different view of
lexical knowledge as implicitly encoded in a distributed memory, that
we intend to characterize more precisely and test in future work.

Concerning the model input, we would like to study whether the
CNLM successes crucially depend on the huge amount
of training data it receives.  Are word priors more important when
learning from smaller corpora? In terms of comparison with human
learning, the Wikipedia text we fed our CNLMs is far from
what children acquiring a language would hear. Future work should
explore character/phoneme-level learning from child-directed speech
corpora. Still, by feeding our networks ``grown-up'' prose, we are
arguably making the job of identifying basic constituents harder than
it might be when processing the simpler utterances of early
child-directed speech \cite{Tomasello:2003}.

As discussed, a rigid word notion is problematic both
cross-linguistically (cf.~polysynthetic and agglutinative languages)
and within single linguistic systems \cite[cf.~the view that
the lexicon hosts units at different levels of the linguistic
hierarchy, from morphemes to large syntactic constructions,
e.g.,][]{Jackendoff:1997,Croft:Cruse:2004,Goldberg:2005}. This study provided a necessary initial check
that word-free models can account for phenomena traditionally
seen as word-based. Future work should test whether this
model can also account for grammatical patterns that are harder to
capture in word-based formalisms, exploring both at a typologically
wider range of languages and a broader set of grammatical tests.



