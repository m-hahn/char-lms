* Introduction

We thank the editor and the reviewers for their insightful comments, that greatly helped us to strentghen the paper. We believe that we present now a stronger, more coherent story than in the first version.

The editor letter and the reviews pointed out that the paper contained too many experiments presented too succintly, and that the general purpose of the study was not clear. Several specific issues were also raised, in particular concerning the phonology and segmentation sections.

The revised version focuses on the question of whether models that have no hard-coded word lexicon and are not explosed to explicit cues to wordhood can handle linguistic phenomena, such as the ones we analyze in the morphology, syntax and semantics sections, that are traditionally seen in linguistics as being word-mediated. We relate this analysis to the current discussion in theoretical linguistiics on the primacy of the notion of word. We have consequently rewritten large parts of the introduction and of the conclusion, to strengthen the theoretical message. We have removed the phonotactic experiments, since they do not address the issue of word-hood. The segmentation section has been completely re-designed and rewritten. It is now the last experimental section of the paper, where it now more naturally fits the general story, by starting to address the question of *how* a "word-less" model can suceed (at least to some degree) in word-mediated tasks.

Following the lead of one reviewer, we searched more thoroughly for "boundary units" that the network might specialize to track word/morpheme boundaries, as found by Kementchedjhieva and Lopez. We were able to find such units, and the segmentation section is now a detailed analysis of their behaviour. We believe that this is both greatly simplifying the methodology, and estabilishing a stronger connections with the general purpose of the paper.

By removing the phonotactic experiments and changing the methodology of the segmentation section, some of the claims and conclusions that the reviewers found most problematic are now gone. Moreover, thanks to the further space afforded by removing some experiments and by the extra pages granted by the editor, we now explain our experiments in more detail. In particular, each experimental section starts now with a thorough discussion of the goal of the experiments and of the methodology we adopt.

We have rewritten the related-work section, being careful to avoid the "defensive" tone that had been noticed by a reviewer. We have further added a new sub-section on the theoretical debate on words in linguistics, as well as a few references that escaped us the first time around.

Finally, we have checked the whole paper for unwarranted claims, and we discuss the mediocre performance of our model in number prediction more thoroughly.

We describe other changes we made below, where we respond in detail to the comments by the editor and the reviewers.

* Response to the Editor

"""The main objections are that the many small insights in the individual experiments do not add up to a concrete claim about what these models learn, and they definitely are not strong enough on their own to hold up the broad claims that frame the paper, which encompass language acquisition, multilinguality, phonology, morphology, syntax, and semantics! See especially reviewer C's comments, which suggest that the paper may actually be clearer with less material, more precisely described; reviewer A's comments, which suggest that the paper should tone down its claims and make them more concrete; and the paper itself, which acknowledges that "our results are preliminary in many ways" (line 967).

In this case, my prescription is simply to present a concrete claim that is carefully supported by a coherent set of experiments. But this prescription is in fact vague: many different subsets of these results could be framed in different ways, possibly requiring different additional sets of supporting experiments. I don't feel it's my place to make that choice for you, so I've given you a (c). But for what it's worth, I suspect that making this paper TACL-worthy would require an amount of work on the short side of the 3-6 month period suggested for a (c) review."""

The paper has now been re-purposed to focus on the study of how word-less models can handle phenomena that are traditioanlly seen as word-mediated. We removed the phonological experiments, that become irrelevant in this perspective, completely re-hauled the segmentation section with new experiments that are both simpler and more intuitive than the original ones, and rewrote the introduction, related work and conclusion to frame our claims more clearly. Thanks to the extra space, we could also discuss all the other experiments in more detail.

* Response to Reviewer A

"""I think the study is worthwhile, but I think the authors need to be far more cautious in the claims they are making about what these models learn. It would be more beneficial to reflect on how these tasks *begin* to inform us about what kinds of linguistic structure language-model trained neural nets can "learn"."""

We have re-focused the paper on the question of whether word-less models can handle some linguistic phenomena that are intuitively word-mediated, and checked the whole paper for unwarranted claims.

"""(1) The choice of languages should be motivated up front. Why English, German and Italian, which are all closely related? Why only three?"""

We now motivate this in the introduction: "We evaluate our character-level networks on a bank of linguistic tests in German, Italian and English. We focus on these languages due to resource availability and ease of benchmark construction. We also think that these well-studied synthetic languages with a clear, orthographically-driven notion of word are a better starting point to test non-word-centric models, compared to agglutinative or polysynthetic languages, where the very notion of what counts as a word is problematic."

Note that, as we briefly discuss in the introduction to the section on the syntactic tests, the nature of character-level models makes it problematic to use existing benchmarks. Thus, any further language to be added requires considerable extra manual effort.

"""(2) The very first evaluation ("Discovering phonological classes") is oddly imprecise and impressionistic. Why should the reader take the authors' word for it that "it definitely suggests that the CNLM has discovered a fair deal about the features organizing the phonological system of the language." This should be replaced with something quantitative or at least more objective, or dropped."""

We dropped the section on phonology.

"""(3) The authors claim to be testing whether the CNLM develops an implicit notion of words, but the testing methodology involves a supervised training step. The paper needs to be much clearer about how this is actually testing whether the unsupervised system has an implicit notion of "word". (Similar remarks hold for the morphology tests.)"""

In the new segmentation experiment, we are focusing on the behaviour of single boundary units. We use training data simply to find a threshold on the value of these units that maximally separates boundary positions from other positions. If the boundary unit did not tend to pick up boundaries, there is no way in which setting a single scalar threshold on its values would lead to anything better than random classification accuracy.

Concerning the morphological experiments, we have now tried to clarify the methdod of "diagnostic classifiers" that we adopted from the recent literature, where it is becoming a standard for analyzing neural mdoels of language (see, e.g., among the references cited in our article, Shi et al., 2016, Adi et al., 2017, Conneau et al., 2018, Hupkes et al., 2018). The main idea of the method is to use activations produced by a pre-trained, fixed model as input features to a classifier for the relevant distinction. As the classifier can only rely on the features provided by the model, if it is able to successfully distinguish the property of interest, it means that the latter is already encoded in the activations provided by the model. Since we use in particular a shallow classifier and extremely small training sets (20 examples in total for part of speech and 30 examples in total for number), we do not see how the classifier could induce the target distinctions if there were not already saliently encoded in the representations provided by the pre-trained model. This is how we explain the approach in the paper:

"We use here the popular method of "diagnostic classifiers" (Hupkes et al. 2017). That is, we treat the hidden activations produced by a CNLM whose weights were fixed after language model training as input features for a shallow (logistic) classifier of the property of interest (e.g., plural vs. singular). If the classifier is successful, this means that the representations provided by the model are encoding the relevant information.  The classifier is deliberately shallow and trained on a small set of examples, as we want to test whether the properties of interests are robustly encoded in the representations produced by the CNLMs, and amenable to a simple linear readout (Fusi et al., 2016). In our case, we want to probe word-level properties in models trained at the character level. To do this, we let the model read each target word character-by-character, and we treat the state of its hidden layer after processing the last character in the word as the model's implicit representation of the word, on which we train the diagnostic classifier."

"""(4) The results of the pluralization study seem quite equivocal. In particular, the fact that the Umlaut plurals aren't properly modeled suggests that it's *not* picking up on an abstract notion of "plural". The paper doesn't seem to acknowledge this sufficiently, either here or especially in the conclusion."""

Thanks for raising this issue. We ran a detailed error analysis of this experiment, and we found a confound in our data: nearly all training plurals contained -e- as a final vowel. When we removed these confounds, we obtained the overall more interpretable results we report in Table 3 of the revised version. Now, on the one hand, the LSTM-based character-level model is clearly above chance even in Umlaut generalization. On the other hand, its performance if far from great in this case. We thus present a more cautious conclusion--there is definitely *some* degree of generalization (new forms in -r, and abve-chance performance for the Umlaut class), but it is not a completely abstract one, as it applies more reliably when generalizing to new affixation patterns, than to a fully non-concatenative process. We discuss this in the section on plurals, but we also get back to it in the conclusion, to hedge our claims. We have also made a general pass through the abstract and the introduction, to tone down any claim that sounded too optimistic given these results.

"""(5) That "case subcategorization" is represented by testing exactly one preposition in one language seems very narrow. Also, unlike German verbs which can be separated from their objects, P-NP sequences are not likely to be broken up, so this seems like something pretty surfacy/sequential and not really convincingly "syntax"."""

We agree that this is a limited test, and we made sure that, in the new version, the preliminary nature of our results is emphasized (we also renamed the section "Prepositional case subcategorization", to stress the limited scope). The need to control for length of stimuli in characters makes the construction of these benchmarks very difficult. Ad the same time, we sampled the nominalized adjextives from a pool of thousands, and used more than 1.5K different sentential contexts. Thus, while it is true that we are only studying one specific case assignment phenomenon here (case assignment by the preposition "mit"), we feel confident that our results for this specific phenomenon are fully general.

We did not understand your second point. In a phrase such as "mit der sehr roten", the form of the final (nominalized) adjective "roten" is determined by the fact that the latter is embedded in a noun phrase governed by the non-adjacent preposition "mit". Moreover, only the full sentential context licenses the syntactic structure in which "mit" is the head of the phrase governing the NP. Finally, as the n-gram and preposition-less controls show, surface statistics cannot account for model behaviour, and some degree of structure-sensitivity is called for. So, while we fully agree that this is a rather simple form of syntax, we are puzzled by the claim that this is not convincingly syntax.

We took various steps to clarify and position the experiment. First, we added a discussion of the general methodology and important methodological issues at the beginning of the section on syntactic dependencies. Second, the part on the subcategorization experiment has been re-written and extended, to explain in more detail and with examples what we test here. As above, in the paper as a whole we make sure we are in any case not making any overly general claim about "syntax".

"""(6) The conclusion seems to over-claim compared to what the paper is actually showing. Most egregiously, I don't think that the sentence completion task establishes knowledge of "basic semantics". The syntactic agreement phenomena results are also somewhat equivocal (see detailed comments below) and the word units results rely on a supervised training step."""

We edited the conclusion (and introduction/abstract) to make our claims more precise and less over-reaching. See our reply above concerning the point on word segmentation, and replies above and below on the syntactic and semantic tests.

"""Sec 2: How does this related work inform the questions you are asking? (The literature review reads as 'defensive', i.e. trying to prove that the work in the paper is novel, rather than situating the work with respect to existing literature.)"""

We rewrote the related work section. Given the new focus we provide in the intro, we added a sub-section on the theoretical on the primacy of the word. We have moreover edited the other subsections, removing any 'defensive' claim, and emphasizing instead what previous work has already contributed to our goals.

"""Sec 2: This paper may also be relevant: Ettinger et al 2018 `Assessing Composition in Sentence Vector Representations'
https://urldefense.proofpoint.com/v2/url?u=https-3A__aclanthology.coli.uni-2Dsaarland.de_papers_C18-2D1152_c18-2D1152&d=DwIBaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=eW1NdU8kpHF5nrq2Z__Rnw&m=8858iCmrOZOuNi3cLSQjugg77jFv3IG_TiJzC3Wh2ZU&s=C8soDo9_d3O47KTapHf4ZBX9C7Ryoz8J8nRZ8n7Mjqw&e="""

Thanks, we added the reference.

"""ln 209 It's not clear to me what "in a localist fashion" means."""

We rephrased, removing this unclear expression.

"""ln 240 Does "We used LSTM cells for WordNLMs" mean something different from "We only tested a word-level LSTM and not a word-level RNN"? If so, what?  Also, why not do the word-level RNN?"""

We rephrased to confirm that this is indeed what we meant. We expect the WordNLM to act as an upper bound for our character-level models, in the word-mediated tasks we consider. For this reason, and because the paper does not focus on the analysis of word-level models per se, we thought it would make sense to directly adopt an LSTM model, as there is a vast literature indicating superiority of LSTMs to vanilla RNNs, both in terms of NLP task performance and in capturing linguistic phenomena. On the other hand, we included a character-level RNN because we are concentrating on character-level models, and thus we think it is interesting to ascertain how much, in this domain, LSTM cells are crucial to good model behaviour, if at all. Given that the paper is already reporting many analyses, and word-level models are only introduced for comparison purposes, we fear that adding results for a word-level RNN would affect the clarity of the paper, without bringing new insights on our specific problem.

"""ln 325 "The LSTM assigns higher probability to the acceptable bi-grams in all but two cases." Are the ratios of "~1" being counted as "higher"? Why?  Similarly the caption to Table 2 says "Values > 1 in bold", but "~1" is in bold (in two places)."""

We rounded these values, so that "~1" really means <1.01. This was an unfortunate choice. We have hover now entirely removed this section from the paper.

"""ln 385 What would be the linguistic basis for wider contexts helping with phoneme classes? (Long-distance phonological phenomena are relatively rare, and none---things like vowel harmony--immediately come to mind for the languages tested.)"""

We agree, and the phonological results would definitely deserve further study. However, as we have now removed this section from the paper, we will leave such study to future work.

"""ln 417 Why 20 characters? Isn't that way longer than most words, even in German?"""

This was an arbitrary threshold, and we found out empirically that changing it did not have much impact on performance. However, this experiment has now been removed, replaced by a simpler one that does not require picking such abritrary hyperparameters.

I AM HERE

=> this might go

ln 475 If you're working from phonological properties, why would fixed
expressions turn up? Is there any reason to believe that in their
orthographic form the internal word boundaries of fixed expressions are less
like other word boundaries?

=> clarify (and this might go)

ln 516 What was the training set used for the Berkeley Parser to be able to
parse German?

=> clarify (done)

ln 546 "unambiguously tagged in the corpus":  I think it would be useful to
remind the reader here that these aren't gold tags but come from TreeTagger
(right?)

=> clarify (done)

Table 5 I don't understand what the last two lines are. Is WordNLM_subs.
without OOV and WordNLM the full test set? If so, then ln 578 "the
word-based model fares better" doesn't seem to make any sense---WordNLM
scores *lowest*.

=> report results with test sets redesigned to only include words in WordNLM vocabulary (and mention this in text)
(I think we decided to instead keep the previous setting, and explain it better)

ln 582 "We study German as it possesses nominal classes that form plural
through different morphological processes" This is also true in Italian!

=> explain

ln 589 Both of the cites given for "German UD treebank" seem to be about the
UD project in general. Surely there's a specific citation for the German UD
treebank that should be included to give those researchers credit for their
work.

=> do (done)

ln 661 "To avoid phrase segmentation ambiguities, we present phrases
surrounded by full stops." I'm not sure what this means. What is the system
presented with at test time? Just a phrase like in (1) (with only one
article)? Why would not having full stops (before and after??) lead to
ambiguity?

=> explain (TODO)

ln 744 "as these often reflect lemmatiziation problems": Are these problems
with TreeTagger, your system, or something else?

=> explain (TODO Michael)

ln 750 When would German ever have discontinuous NPs?

=> explain (TODO Michael)

ln 752 Is it well established that RNNs & LSTMs have the same probabilistic
bias for shorter sequences that e.g. HMMs do?

=> explain (done)

ln 774-776 I found this too terse. What is the n-gram count model? Why omit
the sentence environment?

=> explain (TODO Michael)

ln 778 What stimuli not including the preposition? Where are these
described?

=> explain (rephrased a bit)

4.4.2 If the words occur in the corpus, they presumably occur with their
article, so it's not immediately clear to me that the stimuli don't occur in
the corpus. Perhaps the unattested n-grams are the adj+N combination?

=> explain

ln 835 What does "strong semantic anomaly" mean and how is it checked for?

=> explain

ln 890 Threshold for what? (I couldn't quickly figure out what the 500
occurrence were *of*, nor what to compare to "above").

=> explain

ln 919ff I'm extremely skeptical of the claims about the sentence completion
task. In particular, no language model has information about "syntax,
lexical semantics, world knowledge, and pragmatics" beyond what can be
characterized in purely distributional terms --- i.e. what words share what
kind of distributional similarity with what other words. That will be a
partial reflection of part of speech (syntax-ish) and lexical semantics, but
it is no way "world knowledge". Furthermore, models don't "realize" anything
let alone "that [friend and mistress] are human beings".

=> rephrase and explain

ln 965 "somewhat deeper linguistic templates" seems like an overclaim.

=> explain

ln 990 Why didn't you include polysynthetic and agglutinative languages in
your testing? There are pretty good resources available for Inuktitut and
Turkish, respectively, for example.

=> discuss why

ln 991 "the common view that": This should come with citations. Places to
look are work on Construction Grammar (authors such as Chuck Fillmore and
Paul Kay) and also work by Ray Jackendoff.

=> we also have citations, discuss

ln 13-14 recently reached -> has recently reached
ln 096 as it goes -> as it gets
ln 149 model -> models?
ln 431 ad hoc doesn't need a hyphen
ln 531 can discover about -> can discover -or- can discover information
about
=>(done)
ln 622 I'm not sure what "the latter" is supposed to refer back to.
ln 720 the Universal Dependencies -> the German UD treebank
=>(done)
ln 996 capable to flexibly store -> capable of flexibly storing
=>(done)

=> fix these

**************
**************

B (a):

developers for their ongoing work.

1. Section 4.2 presents results on word segmentation. The paragraph
   starting on line 464 qualitatively investigates the errors made by
   the CNLM trained on Wikipedia test (note: it would be beneficial to
   state Wikipedia right at the beginning of the paragraph, rather
   than at its end). It would though be more interesting if this were
   a comparison between the Bayesian and the CNLM model, rather than
   just analyzing the CNLM.  Because, albeit the fact that "CNLM
   performance is comparable" (ref. to Table 4), a close look reveals
   that there is quiet a gap of the two models in terms of precision
   on inducing lexical word types. A comparative analysis would shed
   some light here, it might be that the LSTM gets frequent types
   right but misses other types, compared to the Bayesian method
   constructed with a lexical bias in mind.

=> this might go; introduce new qualitative analysis (done)

2. For the first analysis (phonological classes induced by the output
   embeddings) results for German only are provided in Figure 1. The
   paper should include plots for all three languages, as there is no
   clear motivation why one was selected. There should be space to
   include all three plots.

=> this might go (done)

3. What really surprised me is the bad performance of the vanilla RNN
   compared to the LSTM on the bigram acceptability judgment task
   (lines 382-383). This is in fact dramatic, as the model only needs
   to consider adjacent characters. At first it seems the model is
   underfit, but then the RNN performs reasonably well on other tasks,
   sometimes even being close to the LSTM (e.g., adj-gender agreement
   on Italian, Table 7) and perplexity scores are reasonable as
   well. Maybe a further discussion in light of training data
   properties and locality of the task might shed some light here (how
   long are the paragraphs the models are trained on?). Finally, what
   is also surprising is that the RNN does not improve with in-domain
   training data for the last task (sentence completion, see line 2 in
   Table 8). Why is the vanilla RNN not improving? Would it help to
   fine-tune on the in-domain data?

=> this might go (done)

4. The paper does a great job in discussing related work. I though
   kept wondering about the difference with Kementchedjhieva & Lopez
   (2018). While overall results are in line (RNN-LMs do capture
   morphological properties), the paper is very brief on reporting an
   interesting divergence: "we could not replicate the result with our
   model" (on a single neuron tracking morpheme boundaries). It would
   be interesting to know if this is due to the different modeling
   setup (e.g., would this also hold for the model trained with white-space,
   footnote 6?) or what other reasons there could be at play.

=> discuss (done)

- Table 3: check F1 score for Italian (should be 59 rather than 60)
=> (this is gone)
- Presentation of results in Table 3 and 4: use of different decimal places.
=> (apparently solved)
- Colored figures are unreadable in b/w printing.
- line 936: in Figure 8 > in Table 8
=> done

=> fix 

**************
**************

C (c):

1. No usable datasets submitted.

=> correct this claim

I think this is an interesting area of inquiry.  The experiments in this
paper are extensive, but sometimes don't seem to fit the intent of the
authors and/or are not clearly explained.  The abstract really focuses on
the idea of removing spaces and still being able to recover words and
morphology, but the experiments veer away from that pretty quickly (starting
with experiment 5 below).

=> revise abstract and general claims as above

In general, there are too many experiments crammed into this paper,
and not enough explanation of the experimental set up, or careful
consideration of results.  This paper is right at the page limit, so I
think the authors should reconsider which experiments are most
telling, and move some of the extraneous ones to supplementary
material.

=> we will remove experiments and add explanation

I can't figure out from the TACL page if TACL allows supplementary
material, but in any case, there's too much in these 10 pages to cover
in the detail required for a reader to understand and be able to
reproduce any of these results.

=> unfortunatley not

1. Remove spaces, how does that affect perplexity/bits-per-char?
I'm not convinced that removing spaces is a good proxy to the word
segmentation problem infants and young children encounter, since they are
exposed to much simpler language (single words, very simple sentences).

=> We have bpc's results with spaces; sure not proxy, add caveat

2. Cluster characters by their embeddings.  Do the cluster represent
phonetics?
This experiment is not repeated (or results are not shown) for the RNN.  No
details are given for how the clustering was run (distance metric?) and the
cutoff for clusters appears to be chosen arbitrarily.

=> remove (done)

3. Identify some acceptable and unacceptable bigrams in each language.
Train on data with both sets removed, and then test if the held out bigrams
are assigned probabilities that are consistent with the
acceptable/unacceptable categorization.
Here, I am very surprised that the RNN did so terribly, to the point where I
wonder if there is a bug in the analysis or code.  If there is no bug, I
think a better explanation for this behavior needs to be brought forward.
For example, perhaps the clustering as in Fig 1 would show that the
phonological categories are not learned by the RNN, which would help to
explain the lack of generalization we're seeing in this experiment (which
requires learning phonological categories).

=> remove (done)

4a. Word segmentation
This experiment is not fully explained.  In particular the context PMI is
unclear to me here, and needs more explanation.  But somehow they are
creating features which they use to predict which characters start words

=> change to probing experiment (done)

4b. A small little experiment with a LDA word segmenting algorithm is
included here, but so little detail is given that we can't draw much of a
conclusion.  It's also trained on a different corpus, so it sticks out a
bit.  Suggest this be put into a supplementary material section with more
details.

=> remove (done)

4c. Error analysis
This is actually fairly interesting and I appreciate this qualitative
account

=> will have to replace (done)

4d. Compare PMI to hierarchical distance
This experiment is really light on details and the accompanying figure 2 HAS
NO LABELS WHATSOEVER.  No axis labels and no legend labels!  There is only
one paragraph actually explaining this experiment, and it's not nearly
enough to understand the results.

=> discuss more in depth (TODO)

At this point we begin to veer off course, and the models seem to be trained
and/or tested on single words, which makes a bit of sense sometimes (e.g.
experiment 5 below which uses the models trained in previous sections) but
not always.

=> explain

5. Nouns vs verbs: can they be classified using the final hidden state of a
pre-trained model after reading the last char?
I don't speak German, but this sentence doesn't make any sense to me
"requiring that they end in -en (German) or -re (Italian) (so that models
can’t rely on the affix for classification), " how would restricting the
suffix (en, re) also restrict the affix?  The baseline here is an
autoencoder LSTM trained on words in isolation.  This seems like a straw
man, if only shown words in isolation this model is missing much of the
context information that help the context-full LSTM tell verbs from nouns.

=> explain (done)

6. Can the model detect number
Here I'm unclear what this has to do with the model trained on space-free
text.  The authors seem to be training on single words? "For the training
set, we randomly selected 15 singulars and plurals from each training
class."  The results show that the CNLM can't generalize to umlaut, but the
explanation is lacking (suffix vs internal root vowel change).  Why? is the
interesting question here.

=> discuss (should be okay now?)

There are many more experiments after this point, and the main themes of my
critiques are the same.  There is not enough information given to fully
understand these experiments (and thus replicating would be impossible).
The figures have NO labels.  There is no careful consideration of results.

=> add (is okay now?)

line 242, the models were not trained until validation accuracy plateaued?
That does not seem standard.  How can we know if these models are fit to
compare against each other if we're not sure they're done training?

=> needs some discussion (still TODO)

The citations for the figures/tables are missing a lot of information.  It's
nice to not have to scan through the text to figure out what each figure is
showing, and many of the important details are left out of the
figures+captions (e.g. the acceptable/unacceptable order in Table 2, what
model is used to do the clustering in figure 1).  This is a little of
personal preference (which is why I include it here in the minor comments),
but it makes for an annoying first skim of the paper if you can't figure out
any of the figures without.  E.g. the caption for Table 5: "word class
accuracy with standard errors. ..."  For what task???

=> add (I think this is good now)

Tables with 9 cells, and 3 numbers per cell are pretty hard to parse e.g.
Table 3/4.  A bar graph with just F1 would be nice, unless the authors
actually think P/R make any contribution (they don't seem to talk about P/R
in any detail).

=> fix (done)

Table 3 just gives P/R/F1, I don't think the claim on line 428 (classify
half the tokens correctly) can be inferred from P or R, rather, one needs
accuracy.

=> remove (done)

Section 4.2 needs some subheaders, there's too much going on here and it's
hard to keep track of what the point of the current experiment is.

=> OK, if it doesn't go

A few tables/figures have STD or bootstrapped CI, but many do not. Would
like to see them consistently throughout

=> do or explain where we do it
=> We do it in those cases where there is a randomized train/test selection for a diagnostic classifier using small training data sets -- i.e., in the two morphological experiments.
   For our other experiments, there is no randomized selection of a small training set. Thus standard errors/CIs would be very small (essentially imperceptible), due to the large number of stimuli we use through the experiments.

Line 616: "as above" there's a lot of stuff above at this point, please
refer to something more concrete

=> OK (done)

Is table 8 really comparing results across corpora?  The top 3 models are
trained on wikipedia, and the bottom on Sherlock Holmes?  This is not a
sound comparison, not sure what we're supposed to take away from this
experiment

-> explain (done)
